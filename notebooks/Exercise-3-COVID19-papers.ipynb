{"cells":[{"cell_type":"markdown","metadata":{"id":"EOqzQUq8griq"},"source":["- Alon Moses 308177815\n","- Guy Attia 305743437"]},{"cell_type":"markdown","metadata":{"id":"vUEk7iRZg2fp"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cafesv8Xgil2"},"outputs":[],"source":["import zipfile\n","from os import mkdir, path\n","from shutil import copyfile\n","import json as js\n","import lzma\n","import scipy as scp\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import pairwise_distances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpkC4BL6C5pr"},"outputs":[],"source":["input_path = path.join('..', 'data')"]},{"cell_type":"markdown","metadata":{"id":"Ds1GBtwqhIVm"},"source":["# Preprocessing\n","This preprocessing functions executed offline due to the large datafiles needed to be loaded into the Google Colab server."]},{"cell_type":"markdown","metadata":{"id":"o25mh6uNhEGY"},"source":["## Metadata File\n","- Unzip the zip file\n","- Load the original metadata csv file\n","- Extract the latest 20K papers ID's from the metadata\n","- Save the metadata of the 20K papers in a new csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wx6Cmj3YH2w6"},"outputs":[],"source":["with zipfile.ZipFile('metadata.csv.zip', 'r') as zip_ref:\n","    zip_ref.extractall('.')\n","\n","df_metadata = pd.read_csv('metadata.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osgLJRAvH4Ya"},"outputs":[],"source":["df_metadata['publish_time'] = pd.to_datetime(df_metadata['publish_time'], format='%Y-%m-%d')\n","df_metadata.dropna(subset=['pdf_json_files'], inplace=True)\n","df_metadata.sort_values(by='publish_time', ascending=False, inplace=True)\n","df_metadata_20k = df_metadata.iloc[:20000]\n","df_metadata_20k = df_metadata_20k.loc[:, ['sha', 'pdf_json_files', 'publish_time']]\n","df_metadata_20k['file_path'] = df_metadata_20k['pdf_json_files'].str.split(';').str[0]\n","df_metadata_20k['first_sha'] = df_metadata_20k['sha'].str.split(';').str[0]\n","df_metadata_20k.to_csv('metadata_20k.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"CwXd3MiNuG2B"},"source":["## Text Files\n","- Unzip the zip file containing all the papers documents\n","- Create new clean folder to hold the relevant 20K papers\n","- Iterate over the 20K metadata file and copy the relevant files to the new folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APlN6y5TuGsX"},"outputs":[],"source":["with zipfile.ZipFile('document_parses.zip', 'r') as zip_ref:\n","    zip_ref.extractall('.')\n","\n","mkdir('20k_papers')\n","\n","for _, row in df_metadata_20k.iterrows():\n","  copyfile(src=row['file_path'], dst=path.join('20k_papers', f'{row[\"first_sha\"]}.json'))"]},{"cell_type":"markdown","metadata":{"id":"yOE9m-JdhM8-"},"source":["# Compression Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZtjHdl_rZtk"},"outputs":[],"source":["def compress_str(text):\n","  text_in_bytes = bytes(text, 'utf-8')\n","  return lzma.compress(text_in_bytes)"]},{"cell_type":"markdown","metadata":{"id":"OdAR7YZ0bTf_"},"source":["## Papers representation\n","Represent each paper as an object holding its ID, Abstract text, and Body text.\n","\n","The class contains also the compression function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sP0urkeeZTez"},"outputs":[],"source":["class Paper:\n","    def __init__(self, paper_id=None, paper_metadata=None, file_path='20k_papers'):\n","      # Get paper-id\n","      if paper_id:\n","        self.paper_id = paper_id\n","      elif paper_metadata:\n","        self.paper_id = paper_metadata['paper_id'] \n","      else:\n","        raise Exception('No paper id')\n","      \n","      # Extract info from json file\n","      with open(path.join(input_path, file_path, f'{paper_id}.json')) as file:\n","        content = js.load(file)\n","        self.bib_entries = content['bib_entries']\n","        self.abstract = []\n","        self.body_text = []\n","        # Abstract\n","        for entry in content['abstract']:\n","            self.abstract.append(entry['text'])\n","        # Body text\n","        for entry in content['body_text']:\n","            self.body_text.append(entry['text'])\n","        self.abstract = '\\n'.join(self.abstract)\n","        self.body_text = '\\n'.join(self.body_text)\n","        self.text_to_compress = f'{self.abstract}\\n{self.body_text}'\n","      \n","      # Extract info from metadata file\n","      if paper_metadata:\n","        self.publish_time = paper_metadata['publish_time']\n","        self.journal = paper_metadata['journal']\n","\n","    def compress_text(self):\n","      self.compressed_text = compress_str(self.text_to_compress)\n","      return self.compressed_text\n","\n","    def __repr__(self):\n","        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYqUYUD2ALC0","outputId":"363fbc67-ebb6-434a-cb2e-0cc504384a59"},"outputs":[],"source":["# For example:\n","example_paper = Paper(paper_id='0a0befc62d8c3da285acc99c45f614dbdccaad10')\n","example_paper.compress_text()\n","print(example_paper)"]},{"cell_type":"markdown","metadata":{"id":"YcMof3cRdMuX"},"source":["## Distance function\n","Function that calculates Normalized Compression Distance (NCD)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1649427104725,"user":{"displayName":"Guy Attia","userId":"03757634315762030656"},"user_tz":-180},"id":"8uNya5dAbS19","outputId":"3b708d5a-4851-4da5-8b0f-05e975407e2e"},"outputs":[],"source":["def ncd(paper1_obj, paper2_obj):\n","  both_papers_text_to_compress = paper1_obj.text_to_compress + paper2_obj.text_to_compress # the concatenation of papers texts\n","\n","  paper1_comp = paper1_obj.compress_text()  # compress paper 1\n","  paper2_comp = paper2_obj.compress_text()  # compress paper 2\n","  both_papers_comp = compress_str(both_papers_text_to_compress)  # compress papers concatenated\n","\n","  ncd = (len(both_papers_comp) - min(len(paper1_comp), len(paper2_comp))) / \\\n","      max(len(paper1_comp), len(paper2_comp))\n","  return ncd\n","\n","# For example:\n","example_paper1 = Paper(paper_id='00a1a252ed0905f37c5c4e0526a3ea448daf5cbf')\n","example_paper2 = Paper(paper_id='00a1e7d4d8608d301d802f413ccb3804edcb60ed')\n","ncd(example_paper1, example_paper2)"]},{"cell_type":"markdown","metadata":{"id":"8j9xp5eUsHop"},"source":["# ML - Solution"]},{"cell_type":"markdown","metadata":{"id":"6PMgElgqngRN"},"source":["## Similarity by Journal\n","Our hypothesis is that compressed papers from the same journal will be relatively closer to each other rather than compressed papers from different journals.\n","To assess our hypothesis we will do the following:\n","- Filter papers from specific journals\n","- Calculate the distances of the papers within the same journals (for each one)\n","- Calculate the distances of the papers from different journals (for each combination)\n","- Compare and analyze the distances results"]},{"cell_type":"markdown","metadata":{"id":"9n7sWEPZALDP"},"source":["### Filter Papers\n","Due to the long running times and the unnecessary need of comparing the entire papers combinations, we filtered the dataset by picking only papers from 10 specific journals containing around 27 papers each."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1649609493424,"user":{"displayName":"Guy Attia","userId":"03757634315762030656"},"user_tz":-180},"id":"k5lJgru8nCag","outputId":"e6b0166b-1a52-4801-9681-1cea3d4e7b64"},"outputs":[],"source":["# Extract the 10 relevant journal names\n","rel_10_journals = df_metadata_20k['journal'].value_counts().head(100).tail(10).index.values\n","print('Journals we are going to test:')\n","display(df_metadata_20k['journal'].value_counts().head(100).tail(10))\n","\n","# Keep the metadata only of the papers from these 10 journals\n","mask = df_metadata_20k['journal'].isin(rel_10_journals)\n","rel_cols = ['sha', 'journal', 'publish_time', 'first_sha']\n","df_metadata_10_journals = df_metadata_20k.loc[mask, rel_cols]\n","df_metadata_10_journals.reset_index(inplace=True)\n","df_metadata_10_journals.rename(columns={'index': 'paper_index'}, inplace=True)\n","display(df_metadata_10_journals.head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZTsYpcYALDT"},"outputs":[],"source":["# Copy only the papers from the 10 relevant journals for easier upload to colab\n","# ** Should be used offline only\n","\n","def copy_10_journals_files():\n","    if not path.isdir(path.join(input_path, '10_journals')):\n","        mkdir(path.join(input_path, '10_journals'))\n","    for s in df_metadata_10_journals['first_sha']:\n","        if not path.isfile(path.join(input_path, '10_journals', f'{s}.json')):\n","            copyfile(src=path.join(input_path, '20k_papers', f'{s}.json'), \n","                     dst=path.join(input_path, '10_journals', f'{s}.json'))\n","\n","copy_10_journals_files()"]},{"cell_type":"markdown","metadata":{"id":"OIhF5yNsohAk"},"source":["### Distance Between 2 Papers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzDTmWBxJhXW"},"outputs":[],"source":["sha_lookup_table = {pi: fs for pi, fs in df_metadata_10_journals[['paper_index', 'first_sha']].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJSqkerfALDW"},"outputs":[],"source":["def calc_2_papers_dist(paper_index_1, paper_index_2):\n","    if paper_index_1 == paper_index_2:\n","        return 0\n","\n","    paper_id_1 = sha_lookup_table[paper_index_1[0]]\n","    paper_id_2 = sha_lookup_table[paper_index_2[0]]\n","    try:\n","        paper_obj_1 = Paper(paper_id=paper_id_1, file_path='10_journals')\n","        paper_obj_2 = Paper(paper_id=paper_id_2, file_path='10_journals')\n","    except:\n","        return 0\n","    return ncd(paper_obj_1, paper_obj_2)"]},{"cell_type":"markdown","metadata":{"id":"zDoA_5KOALDX"},"source":["### Similarity in the same journal\n","Find the distance between the papers of the same journal for each one of the relevant journals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fvuVJhLr8YM"},"outputs":[],"source":["def calc_distances_in_journal(df_jour):\n","    \"\"\"\n","    Calculate a distances matrix for a specific journal. \n","    It will hold the distance between every paper in the specific journal\n","    \"\"\"\n","    # Extract the ids of the papers from the specified journal\n","    papers_ids = df_jour['paper_index'].values\n","    papers_ids_array = np.array(papers_ids.reshape(-1, 1))\n","    \n","    # Calc the pairwise distances\n","    df_dist = pd.DataFrame(pairwise_distances(papers_ids_array, metric=calc_2_papers_dist),\n","                           columns=papers_ids.reshape(-1), index=papers_ids.reshape(-1))\n","    \n","    # Remove duplicates distances due to the squared of the matrix\n","    lower_triu_mask = ~np.triu(np.ones(df_dist.shape)).astype(bool)\n","    df_dist = df_dist.where(lower_triu_mask)\n","    df_dist = df_dist.stack().reset_index()\n","    df_dist.columns = ['paper1_index','paper2_index','distance']\n","    return df_dist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ca2cafjqohsH","scrolled":true},"outputs":[],"source":["# Iterate over the 10 journals and find their internal distances\n","journals_dist_dict = {}\n","\n","for jour in rel_10_journals:\n","    mask = df_metadata_10_journals['journal'] == jour\n","    df_jour = df_metadata_10_journals.loc[mask]\n","    df_dist = calc_distances_in_journal(df_jour)\n","    journals_dist_dict[jour] = df_dist"]},{"cell_type":"markdown","metadata":{"id":"de57Kavrogxu"},"source":["### Similarity between different journals\n","Find the distance between the papers of different journals for each combination of the relevant journals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_ez_tMLnDIK"},"outputs":[],"source":["def calc_distances_between_journal(df_jour1, df_jour2):\n","    papers_ids1 = df_jour1['paper_index'].values.reshape(-1, 1)\n","    papers_ids2 = df_jour2['paper_index'].values.reshape(-1, 1)\n","    \n","    dist_matrix = pairwise_distances(X=papers_ids1, Y=papers_ids2, metric=calc_2_papers_dist)\n","    df_dist = pd.DataFrame(dist_matrix, columns=papers_ids2.reshape(-1), index=papers_ids1.reshape(-1))\n","    df_dist = df_dist.stack().reset_index()\n","    df_dist.columns = ['paper1_index','paper2_index','distance']\n","    return df_dist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYZpJX39ALDf","scrolled":true},"outputs":[],"source":["between_journals_dist_dict = {}\n","for jour1 in top_10_journals:\n","    for jour2 in top_10_journals:\n","        if jour1 != jour2:\n","            mask = df_metadata_10_journals['journal'] == jour1\n","            df_jour1 = df_metadata_10_journals.loc[mask]\n","            mask = df_metadata_10_journals['journal'] == jour2\n","            df_jour2 = df_metadata_10_journals.loc[mask]\n","            df_dist = calc_distances_between_journal(df_jour1, df_jour2)\n","            between_journals_dist_dict[(jour1, jour2)] = df_dist"]},{"cell_type":"markdown","metadata":{"id":"NfZu9ILhALDh"},"source":["### Results Analysis\n","Analyze the distances results by ploting the distances distributions and the differences between in-journal and between-journal distances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8a2bLe9TALDi","outputId":"9f011a98-8be0-49c2-eb30-4d2b24970a37"},"outputs":[],"source":["for jour_name, jour_dist_df in journals_dist_dict.items():\n","    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n","    jour_dist_df['distance'].plot.hist(ax=ax1)\n","    ax1.set_title(f'Distance histogram for journal: {jour_name}')\n","    \n","    jour_dist_df['distance'].plot.kde(ax=ax2)\n","    ax2.set_title(f'Distance KDE for journal: {jour_name}')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auY_8V9gALDk","outputId":"d26366b3-8fcf-4b3b-88e5-b136c6e0985b","scrolled":true},"outputs":[],"source":["for jour_name_tuple, jour_dist_df in between_journals_dist_dict.items():\n","    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n","    jour_dist_df['distance'].plot.hist(ax=ax1)\n","    ax1.set_title(f'Distance histogram between journals: {jour_name_tuple[0]} and {jour_name_tuple[1]}')\n","    \n","    jour_dist_df['distance'].plot.kde(ax=ax2)\n","    ax2.set_title(f'Distance KDE between journals: {jour_name_tuple[0]} and {jour_name_tuple[1]}')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rT3VcX8ALDm","outputId":"af584d4f-3a46-429d-cf96-8a0adfd6d9d8"},"outputs":[],"source":["between_journal_means = []\n","for _, jour_dist_df in between_journals_dist_dict.items():\n","    between_journal_means.append(jour_dist_df['distance'].mean())\n","    \n","in_journal_means = []\n","for _, jour_dist_df in journals_dist_dict.items():\n","    in_journal_means.append(jour_dist_df['distance'].mean())\n","    \n","my_dict = {'in_journal_means': in_journal_means, \n","           'between_journal_means': between_journal_means}\n","\n","_, p_value = scp.stats.ttest_ind(a = in_journal_means, b = between_journal_means)\n","\n","fig, ax = plt.subplots(figsize=(14, 7))\n","ax.boxplot(my_dict.values())\n","ax.set_xticklabels(my_dict.keys())\n","plt.title(f'Distance means for in & between journal papers (T-test p-value = {p_value:.5f})')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FHA7H-2dALDn"},"source":["<b>As we assumed, it's easy to see that the average distance between papers from the same journal is significant (p_value < 0.05) lower than distance between papers from different journals.</b>"]},{"cell_type":"markdown","metadata":{"id":"OJ-s-rGfAXXm"},"source":["### Similarity by joint bib references\n","Find the distance between the papers which reference to same bibliography."]},{"cell_type":"markdown","metadata":{"id":"xTWrkIS9tdPS"},"source":["- Create new bib_entries dictionary which contains for each of the files a list of corresponding files in the dataset. The files in this list will follow the below assumptions-\n","  - The publish time distance between the files will not exceed 100 days.\n","  - The files will have at least one common bibliography entry (reference tto other paper, story, etc.)\n","\n","** We limit the number of files in each file's similarity list to 1000 to due to runtime requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuX3PqFDhFag"},"outputs":[],"source":["compressed_dict = {}\n","caclculate_distance_dict = {}\n","\n","for _, row in df_metadata_20k.iterrows():\n","    paper_id = row['first_sha']\n","    paper = Paper(paper_id = paper_id)\n","    compressed_dict[paper_id] = {}\n","    caclculate_distance_dict[paper_id] = []\n","    \n","    paper.publish_time = row['publish_time']\n","    compressed_dict[paper_id]['publish_time'] = row['publish_time']\n","\n","    bib_list = []\n","    for bib_ref in paper.bib_entries:\n","        bib_list.append(paper.bib_entries[bib_ref]['title'])\n","    compressed_dict[paper_id]['bib_entries'] = bib_list\n","    \n","    i = 0\n","    for compare_paper_id in compressed_dict:\n","        if abs(paper.publish_time.day - compressed_dict[compare_paper_id]['publish_time'].day) > 100: continue\n","        i += 1\n","        if compare_paper_id != paper_id:\n","            bib_list2 = compressed_dict[compare_paper_id]['bib_entries']\n","            intersect = [bib_ref for bib_ref in bib_list if bib_ref in bib_list2]\n","            if len(intersect) > 0:\n","                caclculate_distance_dict[paper_id].append(compare_paper_id)\n","        if i % 1000 == 0:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8RAFupv4TgS"},"outputs":[],"source":["with open('bib_relations.json', 'w') as f:\n","    f.write(json.dumps(caclculate_distance_dict))"]},{"cell_type":"markdown","metadata":{"id":"R5WaFRIuBhDm"},"source":["## Read bib_relations.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F_hxAlDhRuL"},"outputs":[],"source":["with open('bib_relations.json') as f:\n","    related_files_dict = js.loads(f.read())"]},{"cell_type":"markdown","metadata":{"id":"9WI9WKCXGk_v"},"source":["##### Create dictionary with 'file_sha's used as keys, pointing to a list of distances values between the paper in the key and the papers appeared in the bib_reference.json lists."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLlgCBlXhPhB"},"outputs":[],"source":["def create_distance_dict(files_dict, low_bound_of_joint, high_bound_of_joint):\n","    distance_dict = {}\n","    i = 0\n","    for file_1_sha in files_dict:\n","        paper_1_obj = Paper(paper_id=file_1_sha)\n","        if len(files_dict[file_1_sha]) < low_bound_of_joint or len(files_dict[file_1_sha]) > high_bound_of_joint: continue\n","        i += 1\n","        file_1_distances = []\n","        for file_2_sha in files_dict[file_1_sha]:\n","            paper_2_obj = Paper(paper_id=file_2_sha)\n","            distance = ncd(paper_1_obj, paper_2_obj)\n","            file_1_distances.append(distance)\n","            if len(file_1_distances) > 50: break\n","        distance_dict[file_1_sha] = file_1_distances\n","        if len(distance_dict) > 10: break\n","    return distance_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrWxGbOOFSW2"},"outputs":[],"source":["number_of_joint_bib_refs = [20, 30] # We filter by papers with 20 to 30 similar bib references.\n","for i in range(len(number_of_joint_bib_refs)-1):\n","    joined_bib_refs_distance_dict = create_distance_dict(related_files_dict, \n","                                                         low_bound_of_joint=number_of_joint_bib_refs[i],\n","                                                         high_bound_of_joint=number_of_joint_bib_refs[i+1])\n","    with open(f'joint_bib_distances_{number_of_joint_bib_refs[i]}_{number_of_joint_bib_refs[i+1]}.json', 'w') as f:\n","        f.write(js.dumps(joined_bib_refs_distance_dict))    "]},{"cell_type":"markdown","metadata":{"id":"YRDpMB1WHRnR"},"source":["##### For the same files used for calculating distance above, calculate their distance with files which have no joint bib reference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssANj277FSW2"},"outputs":[],"source":["def create_disjoint_distance_dict(files_dict, joined_bib_refs_distance_dict):\n","    distance_dict = {}\n","    for file_1_sha in joined_bib_refs_distance_dict:\n","        file_1_distances = []\n","        paper_1_obj = Paper(paper_id=file_1_sha)\n","        for file_2_sha in files_dict:\n","            if file_2_sha not in files_dict[file_1_sha]:\n","                paper_2_obj = Paper(paper_id=file_2_sha)\n","                distance = ncd(paper_1_obj, paper_2_obj)\n","                file_1_distances.append(distance)\n","                if len(file_1_distances) > 50: break\n","        distance_dict[file_1_sha] = file_1_distances\n","    return distance_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NEbwIjpFSW3"},"outputs":[],"source":["disjoint_bib_refs_distance_dict = create_disjoint_distance_dict(related_files_dict, joined_bib_refs_distance_dict)\n","with open('disjoint_bib_distance.json', 'w') as f:\n","    f.write(js.dumps(disjoint_bib_refs_distance_dict))"]},{"cell_type":"markdown","metadata":{"id":"ul__w3lXFeQM"},"source":["# Plot results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5IKpSPPFSW3","outputId":"a69e95cc-3101-480b-818c-297d8edec915"},"outputs":[],"source":["joint_bib_reference_means = []\n","disjoint_bib_reference_means = []\n","\n","for i, (file_sha, distances) in enumerate(joined_bib_refs_distance_dict.items()):\n","    plt.figure(figsize=(16, 4))\n","    \n","    mean_of_joint_bib_distances = sum(distances)/len(distances)\n","    joint_bib_reference_means.append(mean_of_joint_bib_distances)\n","    plt.hist(distances, density=True, bins=30, label = f'Mean of distances of joint bib references = {mean_of_joint_bib_distances}')\n","    \n","    mean_of_disjoint_bib_distances = sum(disjoint_bib_refs_distance_dict[file_sha])/len(disjoint_bib_refs_distance_dict[file_sha])\n","    disjoint_bib_reference_means.append(mean_of_disjoint_bib_distances)\n","    plt.hist(disjoint_bib_refs_distance_dict[file_sha], density=True, bins=30,label = f'Mean of distances of dis-joint bib references = {mean_of_disjoint_bib_distances}')\n","    \n","    if i == 9: break\n","    \n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yp6mrfq8FSW3","outputId":"1f40a303-6ee0-4acb-86ce-8de51332f5af"},"outputs":[],"source":["my_dict = {'joint_bib_papers': joint_bib_reference_means, \n","           'disjoint_bib_papers': disjoint_bib_reference_means}\n","\n","_, p_value = scp.stats.ttest_ind(a = in_journal_means, b = between_journal_means)\n","\n","fig, ax = plt.subplots(figsize=(14, 7))\n","ax.boxplot(my_dict.values())\n","ax.set_xticklabels(my_dict.keys())\n","plt.title(f'Distance means between papers with joint bib reference & disjoint bib reference (T-test p-value = {p_value:.5f})')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GeDoLw1NFmWp"},"source":["<b>In this case, we see that the average distance between papers which reference to similar bibliography isn't significant (p_value >> 0.05).\n","But we still can observe some lower distance between papers which reference to similar biblio graphy compared to ones doesn't have any joint bib references</b>"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Exercise-3-COVID19-papers.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
